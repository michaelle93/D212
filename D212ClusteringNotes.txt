--Unsupervised learning--
finds patterns in data

k-means clustering

from sklearn.cluster import KMeans
model= KMeans(n_clusters=3)
model.fit(samples)

labels = model.predict(samples)
print(labels)

-cluster labels for new samples-

print(new_samples)
new_labels = model.predict(new_samples)
print(new_labels)

-scatter plot- 

import matplotlib.pyplot as plt
xs = samples[:,0] (calling for variable in the 0th column)
ys = samples[:,2] (calling for variable in the 2nd column)
plt.scatter(xs, ys, c=labels)
plt.show()


from matplotlib import pyplot as plt
#assign columns of new points 
xs=new_points[:,0]
ys=new_points[:,1]
plt.scatter(xs, ys, c=labels)
#assign the cluster centers: centroids
centroids = model.cluster_centers_
#assign the columns of centroids
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]
#make scatter of centroid x, y
plt.scatter(centroids_x, centroids_y, marker='D',
s=50)
plt.show()

--Evaluating a cluster--

aligning labels and species
import pandas as pd
df= pd.DataFrame({'labels': labels, 'species': species})
print(df)

ct= pd.crosstab(df['labels'], df['species'])
print(ct)
--
model = KMeans(n_clusters=3)
labels = model.fit_predict(samples)
df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct= pd.crosstab(df['labels'], df['varieties'])
print(ct)

-inertia measures clustering quality-
	measures how spread out clusters are(low better)
	distance from each sample to centroid of cluster
	after fit(), available as attribute intertia_
	cloose 'elbow' in the inertia plot'

from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(samples)
print(model.interia_)
--
ks = range(1,6)
inertias = []
for k in ks:
	model = KMeans(n_clusters=k)
	model.fit(samples)
	inertias.append(model.inertia_)
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

--visualizing the PCA transformation--

-PCA is a scikit learn component like KMeans or StandardScaler
-fit() learns the transformation from the given data
-transform() applies the learned transformation


from sklearn.decomposition import PCA
model = PCA()
model.fit(samples)
transformed = model.transform(samples)
print(transformed)

-since the PCA rotates the so that mean = 0, the PCA features are not linearly correlated (decorrelation)
-value of 0 means = no linear correlation
-principal components = direction of variance, PCA aligns principal components with axes
- after PCA model is fit() principal component can be obtained with components_ function.
-each row defines displacement from mean

prince(models.components_)

-pearson correlation-

import matplotlib.pyplot as plt
from scipy.stats import pearsonr

width = grains[:,0]
length = grains[:,1]
plt.scatter(width, length)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(width, length)
print(correlation)

-decorrelating measurements with PCA-

from sklearn.decomposition import PCA
model = PCA()
pca_features = model.fit_transform(grains)
xs = pca_features[:,0]
ys = pca_features[:,1]
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(xs, ys)
print(correlation)


-intrinsic dimension-
-scatter plots work only in samples have 2 or 3 features
-PCA identifies intrinsic dimension when samples have any number of features
-intrinsic dimension = number of PCA features with significant variance


import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(samples)
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.ylabel('variance')
plt.xlabel('PCA feature')
plt.show()


-first principal component-

plt.scatter(grains[:,0], grains[:,1])
model = PCA()
model.fit(grains)
mean = model.mean_
first_pc = model.components_[0,:] #gettign the first principal component
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
plt.axis('equal')
plt.show()

-variance of the PCA features=

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()

--Dimension reduction with PCA--
-need to specify how many features to keep
-ex. PCA(n_components=2) #keeps the first two PCA components

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(samples)
transformed = pca.transform(samples)
print(transformed.shape)

import matplotlib.pyplot as plt
xs = transformed[:,0]
ys = transformed[:,1]
plt.scatter(xs, ys, c=species)
plt.show()

#truncatedSCD and csr_matrix
-scikit-learn PCA doesnt support csr_matrix
-scikit-learn uses TrunkcatedSVD instead

from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD(n_components=3)
model.fit(documents) #documents is csr_matrix
transformed = model.transform(documents)

#a tf-idf word frequency array

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
csr_mat = tfidf.fit_transform(documents)
print(csr_mat.toarray())
words = tfidf.get_feature_names()
print(words)

-clustering pipeline-
# Perform the necessary imports
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)

# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
print(df.sort_values('label'))


--non negative matrix factorization--

#follows fit() and transform() pattern
#must specify number of components NMF(n_components=2)
#works with Numpy arrays and with csr_matrix

'tf_idf'
-'tf', frequency of word in document 
-'idf', reduces influence of frequent words

from sklearn.decomposition import NMF
model = NMF(n_components=2)
model.fit(samples)
nmf_features = model.transform(samples)
print(model.components_)
print(nmf_features)

#NMF only works with non negative data, stock market data goes up or down, that is negative

# Import NMF
from sklearn.decomposition import NMF

# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features.round(2))









































































