--Unsupervised learning--
finds patterns in data

k-means clustering

from sklearn.cluster import KMeans
model= KMeans(n_clusters=3)
model.fit(samples)

labels = model.predict(samples)
print(labels)

-cluster labels for new samples-

print(new_samples)
new_labels = model.predict(new_samples)
print(new_labels)

-scatter plot- 

import matplotlib.pyplot as plt
xs = samples[:,0] (calling for variable in the 0th column)
ys = samples[:,2] (calling for variable in the 2nd column)
plt.scatter(xs, ys, c=labels)
plt.show()


from matplotlib import pyplot as plt
#assign columns of new points 
xs=new_points[:,0]
ys=new_points[:,1]
plt.scatter(xs, ys, c=labels)
#assign the cluster centers: centroids
centroids = model.cluster_centers_
#assign the columns of centroids
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]
#make scatter of centroid x, y
plt.scatter(centroids_x, centroids_y, marker='D',
s=50)
plt.show()

--Evaluating a cluster--

aligning labels and species
import pandas as pd
df= pd.DataFrame({'labels': labels, 'species': species})
print(df)

ct= pd.crosstab(df['labels'], df['species'])
print(ct)
--
model = KMeans(n_clusters=3)
labels = model.fit_predict(samples)
df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct= pd.crosstab(df['labels'], df['varieties'])
print(ct)

-inertia measures clustering quality-
	measures how spread out clusters are(low better)
	distance from each sample to centroid of cluster
	after fit(), available as attribute intertia_
	cloose 'elbow' in the inertia plot'

from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(samples)
print(model.interia_)
--
ks = range(1,6)
inertias = []
for k in ks:
	model = KMeans(n_clusters=k)
	model.fit(samples)
	inertias.append(model.inertia_)
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

--visualizing the PCA transformation--

-PCA is a scikit learn component like KMeans or StandardScaler
-fit() learns the transformation from the given data
-transform() applies the learned transformation


from sklearn.decomposition import PCA
model = PCA()
model.fit(samples)
transformed = model.transform(samples)
print(transformed)

-since the PCA rotates the so that mean = 0, the PCA features are not linearly correlated (decorrelation)
-value of 0 means = no linear correlation
-principal components = direction of variance, PCA aligns principal components with axes
- after PCA model is fit() principal component can be obtained with components_ function.
-each row defines displacement from mean

prince(models.components_)

-pearson correlation-

import matplotlib.pyplot as plt
from scipy.stats import pearsonr

width = grains[:,0]
length = grains[:,1]
plt.scatter(width, length)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(width, length)
print(correlation)

-decorrelating measurements with PCA-

from sklearn.decomposition import PCA
model = PCA()
pca_features = model.fit_transform(grains)
xs = pca_features[:,0]
ys = pca_features[:,1]
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(xs, ys)
print(correlation)


-intrinsic dimension-
-scatter plots work only in samples have 2 or 3 features
-PCA identifies intrinsic dimension when samples have any number of features
-intrinsic dimension = number of PCA features with significant variance


import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(samples)
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.ylabel('variance')
plt.xlabel('PCA feature')
plt.show()


-first principal component-

plt.scatter(grains[:,0], grains[:,1])
model = PCA()
model.fit(grains)
mean = model.mean_
first_pc = model.components_[0,:] #gettign the first principal component
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
plt.axis('equal')
plt.show()

-variance of the PCA features=

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()

--Dimension reduction with PCA--
-need to specify how many features to keep
-ex. PCA(n_components=2) #keeps the first two PCA components

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(samples)
transformed = pca.transform(samples)
print(transformed.shape)

import matplotlib.pyplot as plt
xs = transformed[:,0]
ys = transformed[:,1]
plt.scatter(xs, ys, c=species)
plt.show()

#truncatedSCD and csr_matrix
-scikit-learn PCA doesnt support csr_matrix
-scikit-learn uses TrunkcatedSVD instead

from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD(n_components=3)
model.fit(documents) #documents is csr_matrix
transformed = model.transform(documents)

#a tf-idf word frequency array

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
csr_mat = tfidf.fit_transform(documents)
print(csr_mat.toarray())
words = tfidf.get_feature_names()
print(words)

-clustering pipeline-
# Perform the necessary imports
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)

# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
print(df.sort_values('label'))


--non negative matrix factorization--

#follows fit() and transform() pattern
#must specify number of components NMF(n_components=2)
#works with Numpy arrays and with csr_matrix

'tf_idf'
-'tf', frequency of word in document 
-'idf', reduces influence of frequent words

from sklearn.decomposition import NMF
model = NMF(n_components=2)
model.fit(samples)
nmf_features = model.transform(samples)
print(model.components_)
print(nmf_features)

#NMF only works with non negative data, stock market data goes up or down, that is negative

# Import NMF
from sklearn.decomposition import NMF

# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features.round(2))

-NMF learns interpretable parts-
-for documents
	-NMF components represent topics
	-NMF features combine topics into documents
-for images
	-NMF components are parts of images
	- grayscale images = no colors, only shades of gray
	-measures pixel brightness
	-representing values btw 0 and 1 (black=0)
-visualizing sample images-

print(sample) #this will show flat array

bitmap = sample.reshape((2,3)) #this helps put the image in correct array
print(bitmap)
#display the image
from matplotlib import pyplot as plt
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.show()

-NMF document example-
# Import pandas
import pandas as pd

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3]

# Print result of nlargest
print(component.nlargest())

-Grayscale NMF example-

# Import pyplot
from matplotlib import pyplot as plt

# Select the 0th row: digit
digit = samples[0,:]

# Print digit
print(digit)

# Reshape digit to a 13x8 array: bitmap
bitmap = digit.reshape((13, 8))

# Print bitmap
print(bitmap)

# Use plt.imshow to display bitmap
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
plt.show()

-decomposing with NMF digits-
# Import NMF
from sklearn.decomposition import NMF

# Create an NMF model: model
model = NMF(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
   show_as_image(component) 
# Assign the 0th row of features: digit_features
digit_features = features[0,:]

# Print digit_features
print(digit_features)

-using PCA to learn parts of dataset-
# Import PCA
from sklearn.decomposition import PCA

# Create a PCA instance: model
model = PCA(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)
    
--Building recommender systems using NMF--

-apply NMF to the word frequency array-

from sklearn.decomposition import NMF
nmf = NMF(n_components=6)
nmf_features = nmf.fit_transform(articles)

-cosine similarity
	-uses angle btw lines
	-higher values mean more simlar
	-max value is 1, when angle is 0 degress

-calculating cosine simliarities-

from sklearn.preprocessing import normalize
norm_features = normalize(nmf_features)
# if has index 23
current_article = norm_features[23,:]
similarities = norm_features.dot(current_article)
print(similarities)

-labeleling the similarities with article titles-

import pandas as pd
norm_features = normaliz(nmf_features)
df = pd.DataFrame(norm_features, index=titles)
current_article = df.loc['Dog bites man']
similarities = df.dot(current_article) #to calc similarities
print(similarities.nlargest()) #nlargest returns highest cosine similarity

-cosine similarity example-
# Perform the necessary imports
import pandas as pd
from sklearn.preprocessing import normalize

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=titles)

# Select the row corresponding to 'Cristiano Ronaldo': article
article = df.loc['Cristiano Ronaldo']

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())


-make pipeline with normalized NMF features-

# Perform the necessary imports
from sklearn.decomposition import NMF
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline

# Create a MaxAbsScaler: scaler
scaler = MaxAbsScaler()

# Create an NMF model: nmf
nmf = NMF(n_components=20)

# Create a Normalizer: normalizer
normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler, nmf, normalizer)

# Apply fit_transform to artists: norm_features
norm_features = pipeline.fit_transform(artists)

-finding similarities using cosine similarity-
# Import pandas
import pandas as pd

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=artist_names)

# Select row of 'Bruce Springsteen': artist
artist = df.loc['Bruce Springsteen']

# Compute cosine similarities: similarities
similarities = df.dot(artist)

# Display those with highest cosine similarity
print(similarities.nlargest())


from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
scaler = StandardScaler()
kmeans = KMeans(n_cluster=3)
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(scaler, kmeans)
pipeline.fit(samples)

labels = pipeline.predict(samples)









































