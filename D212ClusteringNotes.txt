--Unsupervised learning--
finds patterns in data

k-means clustering

from sklearn.cluster import KMeans
model= KMeans(n_clusters=3)
model.fit(samples)

labels = model.predict(samples)
print(labels)

-cluster labels for new samples-

print(new_samples)
new_labels = model.predict(new_samples)
print(new_labels)

-scatter plot- 

import matplotlib.pyplot as plt
xs = samples[:,0] (calling for variable in the 0th column)
ys = samples[:,2] (calling for variable in the 2nd column)
plt.scatter(xs, ys, c=labels)
plt.show()


from matplotlib import pyplot as plt
#assign columns of new points 
xs=new_points[:,0]
ys=new_points[:,1]
plt.scatter(xs, ys, c=labels)
#assign the cluster centers: centroids
centroids = model.cluster_centers_
#assign the columns of centroids
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]
#make scatter of centroid x, y
plt.scatter(centroids_x, centroids_y, marker='D',
s=50)
plt.show()

--Evaluating a cluster--

aligning labels and species
import pandas as pd
df= pd.DataFrame({'labels': labels, 'species': species})
print(df)

ct= pd.crosstab(df['labels'], df['species'])
print(ct)
--
model = KMeans(n_clusters=3)
labels = model.fit_predict(samples)
df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct= pd.crosstab(df['labels'], df['varieties'])
print(ct)

-inertia measures clustering quality-
	measures how spread out clusters are(low better)
	distance from each sample to centroid of cluster
	after fit(), available as attribute intertia_
	cloose 'elbow' in the inertia plot'

from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(samples)
print(model.interia_)
--
ks = range(1,6)
inertias = []
for k in ks:
	model = KMeans(n_clusters=k)
	model.fit(samples)
	inertias.append(model.inertia_)
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

--visualizing the PCA transformation--

-PCA is a scikit learn component like KMeans or StandardScaler
-fit() learns the transformation from the given data
-transform() applies the learned transformation


from sklearn.decomposition import PCA
model = PCA()
model.fit(samples)
transformed = model.transform(samples)
print(transformed)

-since the PCA rotates the so that mean = 0, the PCA features are not linearly correlated (decorrelation)
-value of 0 means = no linear correlation
-principal components = direction of variance, PCA aligns principal components with axes
- after PCA model is fit() principal component can be obtained with components_ function.
-each row defines displacement from mean

prince(models.components_)

-pearson correlation-

import matplotlib.pyplot as plt
from scipy.stats import pearsonr

width = grains[:,0]
length = grains[:,1]
plt.scatter(width, length)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(width, length)
print(correlation)

-decorrelating measurements with PCA-

from sklearn.decomposition import PCA
model = PCA()
pca_features = model.fit_transform(grains)
xs = pca_features[:,0]
ys = pca_features[:,1]
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()
correlation, pvalue = pearsonr(xs, ys)
print(correlation)


-intrinsic dimension-
-scatter plots work only in samples have 2 or 3 features
-PCA identifies intrinsic dimension when samples have any number of features
-intrinsic dimension = number of PCA features with significant variance


import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(samples)
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.ylabel('variance')
plt.xlabel('PCA feature')
plt.show()


-first principal component-

plt.scatter(grains[:,0], grains[:,1])
model = PCA()
model.fit(grains)
mean = model.mean_
first_pc = model.components_[0,:] #gettign the first principal component
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
plt.axis('equal')
plt.show()

-variance of the PCA features=

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()


















































